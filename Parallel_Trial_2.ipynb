{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Parallel Trial 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chanphoomk/PythonConcurrentWebScraping_1/blob/master/Parallel_Trial_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHlmxyeJn0uy",
        "colab_type": "text"
      },
      "source": [
        "# ***Summary ***\n",
        "\n",
        "After Scraping for on website, with 2 second time stop, the results are shown below.\n",
        "\n",
        "**1 page, 10 Articles and links**\n",
        "* Normal Scraping took around 2.59 second, \n",
        "* Concurrent Scraping took around 2.01 second, \n",
        "* faster than 1.29 times\n",
        " \n",
        "\n",
        "**10 pages, 100 Articles and links**\n",
        "* Normal Scraping took around 25.33 second,\n",
        "* Concurrent Scraping took around 2.073 second,\n",
        "* faster than 12.22 times\n",
        "\n",
        "**50 pages, 500 Articles and links**\n",
        "* Normal Scraping took around 135.07 second,\n",
        "* Concurrent Scraping took around 3.58 second,\n",
        "* faster than 37.73 times\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csUU0rRkICc2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests \n",
        "import time\n",
        "from bs4 import BeautifulSoup as soup\n",
        "\n",
        "def scrape_page(num_page):\n",
        "  #Way to fix missing data in the tag (TRY, EXECPT)\n",
        "  #print(num_page)\n",
        "  result=requests.get(f\"https://thestandard.co/category/news/world/page/{num_page}\")\n",
        "  src = result.content\n",
        "  #To be able to pass HTML content\n",
        "  soup = BeautifulSoup(src,'lxml')\n",
        "\n",
        "  for article in soup.find_all(\"h3\",class_='news-title'):\n",
        "    headline = article.a.text\n",
        "    headline = headline.replace('\\n', '') #Remove new line\n",
        "    news_src = article.find('a')['href']\n",
        "\n",
        "    post_content.append(headline)\n",
        "    link_content.append(news_src)\n",
        "    view_content.append(find_total_view(news_src))\n",
        "\n",
        "  scrape_date(num_page)\n",
        "    \n",
        "def scrape_date(num_page):\n",
        "  result=requests.get(f\"https://thestandard.co/category/news/world/page/{num_page}\")\n",
        "  src = result.content\n",
        "  #To be able to pass HTML content\n",
        "  soup = BeautifulSoup(src,'lxml')\n",
        "  for date_collect in soup.find_all(\"div\",class_='date'):\n",
        "    date = date_collect.text\n",
        "    date = date.replace('\\n', '')\n",
        "    date_content.append(date)\n",
        "\n",
        "def total_page():\n",
        "  result=requests.get(f\"https://thestandard.co/category/news/world\")\n",
        "  src = result.content\n",
        "  #To be able to pass HTML content\n",
        "  soup = BeautifulSoup(src,'lxml')\n",
        "  tt_page = soup.find(\"span\",class_='pages').text\n",
        "  tt_page = int(tt_page.replace('1 of ', ''))\n",
        "  return tt_page"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ExlZb3DIDru",
        "colab_type": "code",
        "outputId": "006c8d8a-d586-4beb-89b7-470560a10f25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# install chromium, its driver, and selenium\n",
        "!apt-get update\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "# set options to be headless, ..\n",
        "from selenium import webdriver\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('--headless')\n",
        "options.add_argument('--no-sandbox')\n",
        "options.add_argument('--disable-dev-shm-usage')\n",
        "# open it, go to a website, and get results\n",
        "#wd = webdriver.Chrome('chromedriver',options=options)\n",
        "#wd.get(\"https://www.website.com\")\n",
        "#print(wd.page_source)  # results"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Wa\r                                                                               \rGet:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [92.5 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [3 \r                                                                               \rGet:4 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [4 InRelease 2,586 B/88.7\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [4 InRelease 2,586 B/88.7\r0% [3 Packages store 0 B] [Connecting to archive.ubuntu.com (91.189.88.142)] [4\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [4 InRelease 5,482 B/88.7\r                                                                               \rHit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "\r0% [Connected to archive.ubuntu.com (91.189.88.142)] [4 InRelease 14.2 kB/88.7 \r0% [5 InRelease gpgv 21.3 kB] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rIgn:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [5 InRelease gpgv 21.3 kB] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [5 InRelease gpgv 21.3 kB] [Waiting for headers] [4 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [8,815 B]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [930 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:17 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,818 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,385 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [852 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [59.3 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [20.1 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [73.6 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,226 kB]\n",
            "Get:24 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [876 kB]\n",
            "Fetched 7,614 kB in 6s (1,232 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 52 not upgraded.\n",
            "Need to get 77.3 MB of archives.\n",
            "After this operation, 264 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 81.0.4044.138-0ubuntu0.18.04.1 [1,095 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 81.0.4044.138-0ubuntu0.18.04.1 [68.9 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 81.0.4044.138-0ubuntu0.18.04.1 [3,231 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 81.0.4044.138-0ubuntu0.18.04.1 [4,079 kB]\n",
            "Fetched 77.3 MB in 10s (8,025 kB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 144433 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_81.0.4044.138-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_81.0.4044.138-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (81.0.4044.138-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnSlPqCgIF8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_total_view(link):\n",
        "    driver = webdriver.Chrome('chromedriver',options=options)\n",
        "    driver.get(\"https://thestandard.co/moon-jae-in-party-won-the-election-in-south-korea/\")\n",
        "    time.sleep(3)\n",
        "\n",
        "    page_html = driver.page_source\n",
        "    #driver.close()\n",
        "    data = soup(page_html,'html.parser') #scan data\n",
        "    span = data.findAll('span', {'class' : 'entry-view'})\n",
        "    span = span[0].findAll('span')\n",
        "\n",
        "    tt_view = span[0].text\n",
        "\n",
        "    return tt_view\n",
        "\n",
        "def scrape_date_from_page(web_link):\n",
        "  result=requests.get(web_link)\n",
        "  src = result.content\n",
        "  soup = BeautifulSoup(src,'lxml')\n",
        "  raw_date = soup.find(\"div\", {'class' : \"meta-date\"})\n",
        "  date = raw_date.find('span').text\n",
        "  return date\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwpXamerIU0b",
        "colab_type": "code",
        "outputId": "9e7ea1ac-d835-49dd-b015-944a9346a819",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "find_total_view('https://thestandard.co/germany-slowly-eases-lockdown/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'979'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pp5xP4JKgev3",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "# ***Normal Scraping***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOPxR8jchpui",
        "colab_type": "code",
        "outputId": "ef6739d2-bad9-48e0-c1fa-b8433620d335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "post_content = []\n",
        "link_content = []\n",
        "\n",
        "start = time.perf_counter()\n",
        "\n",
        "page_to_scrape=50\n",
        "\n",
        "for num_page in range(1,page_to_scrape+1):\n",
        "    scrape_page(num_page)\n",
        "    time.sleep(2)\n",
        "end = time.perf_counter()\n",
        "\n",
        "print(f\"time use for normal scrape: {round(end-start  ,3)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-5ce5a2938e42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnum_page\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpage_to_scrape\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mscrape_page\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_page\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-ff6879bfc49e>\u001b[0m in \u001b[0;36mscrape_page\u001b[0;34m(num_page)\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0msrc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m#To be able to pass HTML content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lxml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"h3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclass_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'news-title'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'BeautifulSoup' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlHo0YMofmMy",
        "colab_type": "code",
        "outputId": "c8f3e205-5270-4476-ac63-3ecc6619f6ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import concurrent.futures\n",
        "\n",
        "#Run for heading and link\n",
        "start = time.perf_counter()\n",
        "\n",
        "post_content = []\n",
        "link_content = []\n",
        "date_content = []\n",
        "view_content = []\n",
        "\n",
        "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "\n",
        "  # page_to_scrape = int(total_page())\n",
        "  page_to_scrape = 1\n",
        "  \n",
        "  pages=[]\n",
        "  for i in range(1,page_to_scrape+1):\n",
        "    pages.append(i)\n",
        "\n",
        "  #pages=[1,2,3,4,5,6,7,8,9,10]\n",
        "  executor.map(scrape_page,pages)\n",
        "  time.sleep(2)\n",
        "    \n",
        "end = time.perf_counter()\n",
        "\n",
        "print(f\"time use: {round(end-start,3)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time use: 2.008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Z0bfKyzKXLZ",
        "colab_type": "text"
      },
      "source": [
        "# ***Data to CSV***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMoGzctIKUNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Insert Data to CSV file\n",
        "\n",
        "# importing pandas as pd  \n",
        "import pandas as pd  \n",
        "   \n",
        "# dictionary of lists  \n",
        "dict = {'date':date_content, 'header': post_content, 'link': link_content}  \n",
        "     \n",
        "df = pd.DataFrame(dict) \n",
        "  \n",
        "# saving the dataframe \n",
        "df.to_csv(r'/content/data.csv', index=False) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}